{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Applied Linear Algebra - 2025 <p>Repository for ALI1-S25 at VIA</p> <p>Checkout the homepage!</p> </p> <p>        Your browser does not support the video tag.      </p>"},{"location":"#course-information","title":"Course information","text":"<ul> <li>Course responsible: Associate Professor Richard Brooks, rib@via.dk</li> <li>5 ECTS (European Credit Transfer System), corresponding to 130 hours of work</li> <li>10 sessions, each with a duration of 4 lessons, starting in week 32</li> <li>Bachelor level course</li> <li>Grade: 7-step scale</li> <li>Type of assessment: 4-hour written exam (see exam description in the menu to the left)</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>It is important that you recap some of your high-school math. Most importantly:</p> <ul> <li>Linear equations</li> <li>Systems of linear equations</li> <li>Vectors and vector operations</li> <li>Differential equations</li> </ul>"},{"location":"#lectures-and-course-organization","title":"Lectures and course organization","text":"<p>The course is scheduled to start Monday August 4 and will continue up until and including August 19. All sessions are from 8:20 to 11:50 in room C03.12, except the very first session which is from 8:20 to 14:20. In general, each session is made up of four activities:</p> <ol> <li>At the beginning of each session, there will be a short recap of the previous session.</li> <li>We then go through the exercises from the previous session.</li> <li>We will go through the theory of the current session.</li> <li>After classes, and before the next session, you will have to solve exercises from the current session.</li> </ol> <p>This then loops back to (1) at the beginning of the next session.</p> <p>There are no mandatory assignments, but it is highly recommended to work on the exercises for each session. No instruction is provided for the exercises so you will have to work on them on your own or form study groups.</p>"},{"location":"#course-content-and-learning-objectives","title":"Course content and learning objectives","text":"<p>Applied Linear Algebra focuses on understanding and applying the core concepts of linear algebra to solve real-world problems. The course explores vector spaces, matrices, eigenvalues, and eigenvectors, emphasising their practical applications in fields such as computer graphics, machine learning, and engineering. The course is designed to provide students with a solid foundation in linear algebra, enabling them to tackle complex problems and develop analytical skills.</p> <p>Learning Objectives</p> <ul> <li>Vectors and Vector Spaces: Understand the fundamental concepts of vectors, vector operations, and vector spaces. Learn to interpret vectors algebraically and geometrically, and reason about spans, bases, dimensions, and linear independence.</li> <li>Matrix Algebra: Perform matrix operations including addition, multiplication, and inversion. Understand matrices as transformations and systems of linear equations, and learn to use matrices for practical problem solving.</li> <li>Determinants and Invertibility: Compute determinants of matrices and understand their geometric and algebraic significance. Use determinants to assess matrix invertibility and to solve linear systems.</li> <li>Linear Transformations and Eigenanalysis: Understand linear transformations through matrix multiplication. Learn to find eigenvalues and eigenvectors, diagonalise matrices when possible, and interpret eigenanalysis in practical contexts.</li> <li>Systems of Differential Equations: Apply linear algebra techniques to solve systems of first-order differential equations. Understand the connection between eigenvalues, eigenvectors, and the behaviour of dynamic systems.</li> <li>Orthogonality and Least Squares: Explore orthogonality in vector spaces, apply the Gram-Schmidt process, and solve least squares problems. Understand projections and their role in approximating inconsistent systems and fitting models to data.</li> <li>Symmetric Matrices, SVD, and PCA: Analyse symmetric matrices, perform Singular Value Decomposition (SVD), and understand its application to data reduction and Principal Component Analysis (PCA).</li> <li>Applications of Linear Algebra: Gain hands-on experience solving real-world problems by applying linear algebra techniques across disciplines such as engineering, computer science, and data analysis.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<p>Lay: Lay, David C. Linear Algebra and its applications, 4<sup>th</sup> edition. (e-book, up to students to retrieve a copy). All chapters and exercises referenced will be to the 4<sup>th</sup> edition. Make sure you have the correct edition or else the exercise numbers will not match.</p> <p>Note, for each lesson I have uploaded the presentations that accompany the book. I will in no way use these during classes and they are merely uploaded for your notes/convenience. The notes that I have uploaded are an electronic version of my personal lecture notes and contain most of the (relevant) material for the sections in questions.</p> <p>Non-session specific resources such as the exercises from the book, solutions, old exam cases, etc. can be found her:</p> <p>General Resources ALI</p> <p>This folder is always accessible in the menu to the left.</p> <p>The Wiseflow code for all flows that are used during the course is always 0000. This is not the code for the actual exam in June, though.</p> <p>Suggested online resources can be found in the menu to the left. These are not mandatory, but they can be useful for some students.</p> <p>Make sure you install a working version of Jupyter Notebook and Python version 3.7 or higher. You can choose whichever IDE you want to work in as long as it can handle Jupyter Notebooks. Installing VS Code with a Jupyter Notebook extension seems to be a popular choice.</p>"},{"location":"#historical-notes","title":"Historical Notes","text":"<p>Applied Linear Algebra was first offered in 2014 and is scheduled to be taught 1\u20132 times per year. The course responsible is Richard Brooks (RIB) who has been the only lecturer teaching the course.</p>   **Grade Distribution 2024 (includes re-exams from 2023)**  | Grade | Count | |:-----:|:-----:| | 12    | 1     | | 10    | 8     | | 7     | 15    | | 4     | 9     | | 02    | 2     | | 00    | 11    | | -3    | 1     |"},{"location":"01_Introduction_to_Linear_Algebra/","title":"01 Introduction to Linear Algebra","text":"Introduction to Linear Algebra"},{"location":"01_Introduction_to_Linear_Algebra/#session-material","title":"Session Material:","text":"<p>Lay: 1.1-1.5  + 1.7-1.10 (1.8-1.10 self-study)  </p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"01_Introduction_to_Linear_Algebra/#session-description","title":"Session Description","text":"<p>We'll begin with systems of linear equations, what they are, how to represent them with matrices, and how to solve them using row reduction. You'll learn key row operations, how to reach echelon forms, identify pivots, and determine basic and free variables. We'll cover when systems have unique, infinite, or no solutions, revisiting concepts like Theorem 2.</p> <p>Next, we introduce vectors: linear combinations, spans, and how vector equations relate to linear systems, with geometric intuition in 2D and 3D. We'll examine linear dependence and independence, identifying redundant vectors and linking this to simple homogeneous systems (Theorems 7\u20139). Finally, we'll introduce linear transformations, how matrices map vectors between spaces, laying a vital foundation for what's to come.</p>"},{"location":"01_Introduction_to_Linear_Algebra/#key-concepts","title":"Key Concepts","text":"<ul> <li>Linear Systems</li> <li>Matrices</li> <li>Row Reduction</li> <li>Vectors</li> <li>Span</li> <li>Linear Dependence</li> <li>Linear Transformations</li> </ul>"},{"location":"01_Introduction_to_Linear_Algebra/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"02_Matrix_Algebra/","title":"02 Matrix Algebra","text":"Matrix Algebra"},{"location":"02_Matrix_Algebra/#session-material","title":"Session Material:","text":"<p>Lay: 2.1-2.7 (2.4 + 2.6-2.7 self-study)</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"02_Matrix_Algebra/#session-description","title":"Session Description","text":"<p>Now that we\u2019ve learned to represent and solve systems with matrices, this session focuses on doing algebra with matrices themselves. We\u2019ll start with basic operations like addition, scalar multiplication, and then tackle matrix multiplication \u2014 both its definition and the practical row-column rule.</p> <p>We\u2019ll explore special matrices (zero, identity, diagonal), powers, and transposes, and discuss how matrix operations differ from regular arithmetic. Key theorems (1\u20133 for operations, 5\u20137 for inverses) will guide us through the rules. A central idea is the invertible matrix and to find it using row reduction.</p> <p>We\u2019ll finish with the Invertible Matrix Theorem (Theorem 8), which ties together many core ideas.</p>"},{"location":"02_Matrix_Algebra/#key-concepts","title":"Key Concepts","text":"<ul> <li>Matrix Operations</li> <li>Matrix Multiplication</li> <li>Special Matrices</li> <li>Matrix Transpose</li> <li>Matrix Inverse</li> <li>Invertible Matrices</li> <li>Invertible Matrix Theorem</li> <li>Matrix Algebra</li> </ul>"},{"location":"02_Matrix_Algebra/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"03_Determinants/","title":"03 Determinants","text":"Determinants"},{"location":"03_Determinants/#session-material","title":"Session Material:","text":"<p>Lay: 3.1-3.3</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"03_Determinants/#session-description","title":"Session Description","text":"<p>This session introduces the concept of the determinant of a matrix. We will start by defining and learning how to compute the determinant for 2x2 matrices, and then extend this to 3x3 matrices and  \\(n \\times n\\) matrices, using a systematic method.</p> <p>We will explore the basic properties of the determinant, including how row operations affect its value. A central result that will be highlighted is the relationship between the determinant and a matrix's invertibility \u2013 a matrix is invertible if and only if its determinant is non-zero.</p> <p>Depending on the precise content of the sections, we may also touch upon the geometric interpretation of the determinant, such as its connection to area or volume.</p>"},{"location":"03_Determinants/#key-concepts","title":"Key Concepts","text":"<ul> <li>Determinant</li> <li>Calculating 2x2 Determinants</li> <li>Calculating 3x3 Determinants</li> <li>Properties of Determinants</li> <li>Determinant and Invertibility</li> <li>Geometric Interpretation (Area/Volume)</li> </ul>"},{"location":"03_Determinants/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"04_Vector_Spaces/","title":"04 Vector Spaces","text":"Vector Spaces"},{"location":"04_Vector_Spaces/#session-material","title":"Session Material:","text":"<p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"04_Vector_Spaces/#session-description","title":"Session Description","text":"<p>Moving beyond just vectors in \\(\\mathbb{R}^n\\), this session introduces the more general idea of \"vector spaces\" \u2013 abstract collections of objects that behave like vectors. We'll look at special subsets within these spaces called \"subspaces.\" A major focus will be on two important subspaces connected to any matrix: the \"Null Space\" (all the vectors that get mapped to zero) and the \"Column Space\" (everything that can be reached by combining the matrix's columns). We'll also touch on how these relate to linear transformations.</p> <p>A crucial concept we'll build up to is finding the most efficient way to describe or generate a space \u2013 this leads us to the idea of a \"basis,\" which is a minimal set of vectors that spans the entire space and is linearly independent. Once we have a basis, we can define unique \"coordinate systems\" within the space. We'll then use the basis to define the \"dimension\" of a vector space \u2013 essentially, how many independent directions you need to move within it. Finally, we'll define the \"rank\" of a matrix, which is closely tied to the dimension of its column space and tells us a lot about the structure of the matrix and related linear systems.</p>"},{"location":"04_Vector_Spaces/#key-concepts","title":"Key Concepts","text":"<ul> <li>Vector Spaces</li> <li>Subspaces</li> <li>Null Space</li> <li>Column Space</li> <li>Basis</li> <li>Coordinate Systems</li> <li>Dimension</li> <li>Rank</li> </ul>"},{"location":"04_Vector_Spaces/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"05_Eigenvalues/","title":"05 Eigenvalues","text":"Eigenvalues"},{"location":"05_Eigenvalues/#session-material","title":"Session Material:","text":"<p>Lay: \u200b5.1-5.3</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"05_Eigenvalues/#session-description","title":"Session Description","text":"<p>This session introduces some of the most powerful and fascinating concepts in linear algebra: eigenvalues and eigenvectors! We'll explore what these special vectors (eigenvectors) and scalars (eigenvalues) are \u2013 basically, vectors that only get scaled (not changed in direction) when a matrix transformation is applied. We'll learn how to find eigenvalues by solving the \"characteristic equation\" and then find the corresponding eigenvectors by solving a system of equations.</p> <p>Understanding the set of all eigenvectors for a specific eigenvalue (the \"eigenspace\") is also key. Finally, we'll tackle the concept of \"diagonalization\" \u2013 a super useful technique where we can transform certain matrices into simpler diagonal forms using their eigenvectors and eigenvalues. This process, often called similarity transformation, simplifies many calculations and reveals deeper properties of the matrix and the transformation it represents.</p>"},{"location":"05_Eigenvalues/#key-concepts","title":"Key Concepts","text":"<ul> <li>Eigenvalues</li> <li>Eigenvectors</li> <li>Characteristic Equation</li> <li>Eigenspaces</li> <li>Diagonalization</li> <li>Matrix Similarity</li> </ul>"},{"location":"05_Eigenvalues/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"06_Differential_Equations/","title":"06 Differential Equations","text":"Differential Equations"},{"location":"06_Differential_Equations/#session-material","title":"Session Material:","text":"<p>Lay: 5.7</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"06_Differential_Equations/#session-description","title":"Session Description","text":"<p>We've learned about eigenvalues and eigenvectors and how to diagonalize matrices. In this session, we'll see a powerful application of these ideas to the real world: solving systems of linear differential equations! We'll discover how the eigenvalues and eigenvectors of a matrix connected to the system give us the fundamental building blocks for the solutions.</p> <p>We'll focus on understanding how these concepts help us find the general solution to systems of equations that describe things changing over time, like population models or coupled systems. The eigenvalues will relate to the exponential growth or decay rates, and the eigenvectors will define the directions of these changes. While this is just an introduction, it highlights how abstract linear algebra tools are essential for tackling dynamic problems in science and engineering.</p>"},{"location":"06_Differential_Equations/#key-concepts","title":"Key Concepts","text":"<ul> <li>Systems of Differential Equations</li> <li>Applications of Eigenvalues</li> <li>Applications of Eigenvectors</li> <li>Linear Differential Equations</li> <li>Solution Structure</li> <li>Exponential Solutions</li> </ul>"},{"location":"06_Differential_Equations/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"07_Orthogonality_I/","title":"07 Orthogonality I","text":"Orthogonality I"},{"location":"07_Orthogonality_I/#session-material","title":"Session Material:","text":"<p>Lay: 6.1-6.3</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"07_Orthogonality_I/#session-description","title":"Session Description","text":"<p>Let's add some geometry back into our vector spaces! This session introduces the idea of the \"inner product\" (or dot product in \\(\\mathbb{R}^n\\)) as a way to measure things like the \"length\" of a vector and the \"distance\" between vectors. Most importantly, it gives us a precise way to define \"orthogonal\" (or perpendicular) vectors. We'll explore orthogonal complements \u2013 the set of all vectors truly perpendicular to a given subspace, and see how they connect fundamental spaces we've already studied.</p> <p>Building on this, we'll look at \"orthogonal sets\" of vectors, where every pair is perpendicular. These sets are really special because they're automatically linearly independent, and if they also span the space, they form an \"orthogonal basis\" \u2013 which makes finding coordinates ridiculously easy! Finally, we'll learn how to \"orthogonally project\" a vector onto a subspace. Think of this as finding the closest point in that subspace to the original vector, a super useful tool with lots of applications (related to the Best Approximation Theorem).</p>"},{"location":"07_Orthogonality_I/#key-concepts","title":"Key Concepts","text":"<ul> <li>Inner Product / Dot Product</li> <li>Vector Length and Distance</li> <li>Orthogonal Vectors</li> <li>Orthogonal Complements</li> <li>Orthogonal Sets</li> <li>Orthogonal Bases</li> <li>Orthogonal Projection</li> <li>Best Approximation</li> </ul>"},{"location":"07_Orthogonality_I/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"08_Orthogonality_II/","title":"08 Orthogonality II","text":"Orthogonality II"},{"location":"08_Orthogonality_II/#session-material","title":"Session Material:","text":"<p>Lay: \u200b\u200b\u200b6.4-6.6</p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"08_Orthogonality_II/#session-description","title":"Session Description","text":"<p>Building on our understanding of orthogonal vectors and bases, this session focuses on powerful techniques and applications related to orthogonality. First, we'll learn the \"Gram-Schmidt process\" \u2013 a step-by-step method to turn any basis for a subspace into an orthogonal (or orthonormal) one. This is a fundamental algorithm! Related to Gram-Schmidt is the \"QR factorization,\" a way to break down certain matrices into a product of a matrix with orthonormal columns and an upper triangular matrix.</p> <p>Then, we'll tackle a super common problem in real-world data: what do you do when a linear system \\(A\\mathbf{x}=\\mathbf{b}\\) has no exact solution? We'll introduce \"least-squares\" solutions \u2013 the 'best possible' approximate solutions that minimize the error. Geometrically, finding the least-squares solution involves projecting the vector \\(\\mathbf{b}\\) onto the column space of matrix \\(A\\). We'll see how these least-squares ideas are applied, especially in fitting models to data (like finding the 'best' line through a set of points).</p>"},{"location":"08_Orthogonality_II/#key-concepts","title":"Key Concepts","text":"<ul> <li>Gram-Schmidt Process</li> <li>Orthogonalization</li> <li>QR Factorization</li> <li>Least Squares</li> <li>Inconsistent Systems</li> <li>Projections</li> <li>Data Fitting</li> </ul>"},{"location":"08_Orthogonality_II/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"09_Symmetric_Matrices_SVD_and_PCA/","title":"09 Symmetric Matrices, SVD, and PCA","text":"Symmetric Matrices, SVD, and PCA"},{"location":"09_Symmetric_Matrices_SVD_and_PCA/#session-material","title":"Session Material:","text":"<p>Lay: \u200b\u200b\u200b\u200b7.1+7.4-7.5  </p> <p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"09_Symmetric_Matrices_SVD_and_PCA/#session-description","title":"Session Description","text":"<p>This session dives into some advanced and incredibly useful matrix factorizations. We start by looking at special matrices \u2013 the \"symmetric matrices\" (where A equals its transpose) \u2013 and discover that they have remarkable properties, particularly that they can always be \"orthogonally diagonalized\" (meaning we can find an orthonormal basis of eigenvectors). This fundamental result is captured by the \"Spectral Theorem.\"</p> <p>Then, we move to a factorization that applies to any matrix, not just square or symmetric ones: the \"Singular Value Decomposition\" (SVD). We'll learn about \"singular values\" and \"singular vectors\" and how they provide a powerful way to understand the structure and geometric action of any linear transformation. Finally, we'll explore some of the many real-world \"applications of SVD,\" seeing how this decomposition is essential in areas like data analysis, image processing (for compression), and even in finding the best approximate solutions to systems using the \"pseudoinverse.\"</p>"},{"location":"09_Symmetric_Matrices_SVD_and_PCA/#key-concepts","title":"Key Concepts","text":"<ul> <li>Symmetric Matrices</li> <li>Orthogonal Diagonalization</li> <li>Spectral Theorem</li> <li>Singular Value Decomposition (SVD)</li> <li>Singular Values</li> <li>Singular Vectors</li> <li>Applications of SVD</li> <li>Pseudoinverse</li> </ul>"},{"location":"09_Symmetric_Matrices_SVD_and_PCA/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"10_Recap_and_Conclusion/","title":"10 Recap and Conclusion","text":"Recap and Conclusion"},{"location":"10_Recap_and_Conclusion/#session-material","title":"Session Material:","text":"<p>Recap and Exercises</p> <p>Session Notes</p> <p>Session Material</p>"},{"location":"10_Recap_and_Conclusion/#session-description","title":"Session Description","text":"<p>Alright, this is our final scheduled session! We've covered a massive amount of material in applied linear algebra, building from solving basic systems all the way through vector spaces, transformations, eigenvalues, orthogonality, and powerful factorizations like the SVD.</p> <p>This session is purely dedicated to you. It's a recap and Q&amp;A session. Bring questions you have about any topic we've touched on throughout the course. We'll work through them together, and I'll do my best to clarify any concepts or problems that are still unclear. This is your chance to solidify your understanding and ensure you're ready for the final exam.</p>"},{"location":"10_Recap_and_Conclusion/#key-concepts","title":"Key Concepts","text":"<ul> <li>Recap of Key Concepts</li> <li>Q&amp;A Session</li> <li>Final Exam Preparation</li> <li>May be used as a buffer session for any topics that need more time</li> </ul>"},{"location":"10_Recap_and_Conclusion/#exercises","title":"Exercises","text":"<p>Exercise 1</p> <p>Exercise 2</p> <p>Exercise 3</p> <p>Exercise 4</p> <p>Exercise 5</p> <p>Exercise 6</p> <p>Exercise 7</p> <p>Exercise 8</p> <p>Exercise 9</p> <p>Exercise 10</p>"},{"location":"Sessions/","title":"Sessions","text":"<p>Click on a session to the left (or below) to access a plan of a specific session and additional resources for that session. All sessions are scheduled from 8:20 to 11:50 in room C03.12, except the first session, which is from 8:20 to 14:20. The sessions are as follows:</p>   | Session | Date                 | Topic                        | | ------- | :----:               | ---------------------------- | | 01      | 4 Aug 08:20 \u2013 14:20  | [Introduction to Linear Algebra](/ALI1_25/01_Introduction_to_Linear_Algebra/) | | 02      | 5 Aug 08:20 \u2013 11:50  | [Matrix Algebra](/ALI1_25/02_Matrix_Algebra/) | | 03      | 6 Aug 08:20 \u2013 11:50  | [Determinants](/ALI1_25/03_Determinants/) | | 04      | 7 Aug 08:20 \u2013 11:50  | [Vector Spaces](/ALI1_25/04_Vector_Spaces/) | | 05      | 8 Aug 08:20 \u2013 11:50  | [Eigenvalues](/ALI1_25/05_Eigenvalues/) | | 06      | 11 Aug 08:20 \u2013 11:50 | [Differential Equations](/ALI1_25/06_Differential_Equations/) | | 07      | 12 Aug 08:20 \u2013 11:50 | [Orthogonality 1](/ALI1_25/07_Orthogonality_I/) | | 08      | 13 Aug 08:20 \u2013 11:50 | [Orthogonality 2](/ALI1_25/08_Orthogonality_II/) | | 09      | 14 Aug 08:20 \u2013 11:50 | [Symmetric Matrices, SVD and PCA](/ALI1_25/09_Symmetric_Matrices_SVD_and_PCA/) | | 10      | 19 Aug 08:20 \u2013 11:50 | [Recap and Conclusion](/ALI1_25/10_Recap_and_Conclusion/) |"},{"location":"blog/","title":"Blog","text":""},{"location":"pages/exam/","title":"Exam","text":"Exam"},{"location":"pages/exam/#exam-prerequisites","title":"Exam prerequisites:","text":"<p>None</p>"},{"location":"pages/exam/#exam-type","title":"Exam type","text":"<p>The exam has two parts:</p> <ul> <li>The first part is a Flowlock exam in Wiseflow.</li> <li>The second part is a Wiseflow exam without Flowlock. The second part must be completed in the Jupyter Notebook environment and the answers must be submitted in Wiseflow.</li> </ul> <p>Part 1 has a duration of 3 hours and part 2 has a duration of 1 hour. The exam has a total duration of 4 hours.  The student will not be able to access the second part before the first part is concluded. Part 1 weighs 75% and Part 2 weighs 25% in the final grade.</p>"},{"location":"pages/exam/#tools-allowed","title":"Tools allowed","text":"<p>In the first part the students are allowed to use any notes, books, and/or other written/printed material and will have access to PDF files on their laptop.</p> <p>The students may bring their own calculator.</p> <p>In the second part all supplementary materials and aids are allowed, e.g., using a computer as a reference work.</p> <p>It is not allowed, however, to use AI-tools such as Copilot, ChatGPT, Bing, etc. Communication of any sort is not allowed during the exam and will lead to expulsion of all involved parties from the exam.</p>"},{"location":"pages/exam/#re-exam","title":"Re-exam","text":"<p>Re-exams may be oral.</p>"},{"location":"pages/faq/","title":"FAQ","text":"FAQ"},{"location":"pages/faq/#general-information","title":"General information","text":"Is there a FAQ for the course? <p>Yes, you are looking at it! This FAQ is designed to answer some of the most common questions about the course. If you have a question that is not answered here, please feel free to contact the course responsible, Richard Brooks</p> What is the course about? <p>The course covers an introduction to linear algebra and its applications, particularly its relation to machine learning and computer graphics. In depth description can be found in the course description or by going through the description of each session in the Sessions menu.</p> How is the course related to the study program? <p>The course mostly relates to the study program by providing a foundation for understanding and applying linear algebra in the context of engineering, especially in the field of data science and machine learning as well as computer graphics.</p> What are the prerequisites for the course? <p>It is important that you recap some of your high-school math. Most importantly:</p> <ul> <li>Linear equations</li> <li>Systems of linear equations</li> <li>Vectors and vector operations</li> <li>Differential equations</li> </ul> Who should take this course? <p>The course is intended for students who are interested in learning about linear algebra and how to apply it in the context of engineering. It is very useful for students who are interested in data science and machine learning.  The course is mandatory for some Master's programs.</p> Is attendance mandatory? <p>Attendance is not mandatory, but it is highly recommended. The course is designed to be interactive and hands-on, so attending the lectures will help you understand the material better. If you are unable to attend a lecture, please make sure to catch up on the material covered in class.</p>"},{"location":"pages/faq/#who-to-contact","title":"Who to contact?","text":"Who should I contact if I have questions about the course content? <p>You can contact the course responsible, Richard Brooks, if you have questions about the course content.</p> Who should I contact if I have questions about the exam? <p>You should always contact the Study Service if you have questions about the exam.</p> Who should I contact if I have questions about the schedule? <p>You can contact our scheduler if you have questions about the schedule.</p> Who should I contact if I have scheduling conflicts? <p>You can contact our scheduler if you have questions about scheduling conflicts.</p> Who should I contact if I want to know whether this course is mandatory for a Master's program or if it satisfies a specific requirement? <p>You can contact the Study Councillor.</p>"},{"location":"pages/faq/#exam-and-assessment","title":"Exam and assessment","text":"What type of exam will conclude the course <p>Please see the \"Exam\" section in the menu to the left for detailed information about the exam.</p> When will the exam and re-exam be held? <p>The exam is held August 22 and the re-exam is August 29. Feel free to contact the Study Service for more information.</p> What is the grading scale for the course? <p>The grading scale for the course is the 7-point grading scale.</p> How is the final grade calculated? <p>The final grade is calculated based on the exam. The exam has two parts: a Flowlock exam in Wiseflow and a Wiseflow exam without Flowlock. Part 1 weighs 75% and Part 2 weighs 25% in the final grade. Please see the \"Exam\" section in the menu to the left for more information.</p> What is the re-exam procedure? <p>Re-exams may be oral. Please see the \"Exam\" section in the menu to the left for more information.</p> What happens if I fail the exam? <p>If you fail the exam, you will have the opportunity to take a re-exam. Please see the \"Exam\" section in the menu to the left for more information.</p> What happens if I fail the re-exam? <p>If you fail the re-exam, you will have to wait until the course is held again to retake the exam. In special cases, we may be able to offer you an oral re-exam. Please see the \"Exam\" section in the menu to the left for more information.</p> How many percentage points do I need to pass the exam? <p>The exam is graded on a 7-point grading scale. To pass the exam, you need a grade of 02 or higher. In order to obtain a 02, you need to score at least 50% of the total points on the exam. This score may vary from year to year, but is never higher than 50%.</p>"},{"location":"pages/faq/#resources","title":"Resources","text":"Is the course book mandatory? <p>The course book is not mandatory, but I do recommend finding some resources to help you understand the material. The course book is a good resource, but there are many other resources available online. Please also see the Online Resources section in the menu to the left.</p> Is Python mandatory? <p>Python is mandatory for the course. You will need to use Python to complete the assignments and the exam. If you are not familiar with Python, I recommend finding some resources to help you learn the basics.</p> Is Jupyter Notebook mandatory? <p>Jupyter Notebook is mandatory for the course. You will need to use Jupyter Notebook to complete the assignments and the exam. You can install a plugin in VSCode to run Jupyter Notebooks.</p> Is the course material available online? <p>Yes, the course material is available online. You can find all material by navigating the menu to the left.</p> Is there a recommended study plan? <p>Yes, I recommend following the study plan outlined in the course material. You can find the study plan in the course material by navigating the menu to the left.</p> Where do I find material such as old exam cases, solutions, and other resources? <p>You can find all material by navigating the menu to the left, see \"General Resources ALI\".</p> Are there any additional resources available? <p>Yes, there are many additional resources available online. Please also see the Online Resources section in the menu to the left.</p> What is the Wiseflow code? <p>The Wiseflow code for the course is 0000. However, at the exam you will be given specific codes.</p>"},{"location":"pages/online_resources/","title":"Online Resources","text":"Recommended Online Resources <p>Below are some recommended online resources that can help you with the course material. These resources are not mandatory, but they can be very helpful in understanding the concepts covered in the course.</p>"},{"location":"pages/online_resources/#gilbert-strang-mit-1806-linear-algebra","title":"Gilbert Strang: MIT 18.06 Linear Algebra","text":"<p>This is the OG of linear algebra courses and the one I used when I had to prepare for this course when it ran first time back in 2014. The course is available on MIT OpenCourseWare. It includes lecture notes, assignments, and exams. The course is taught by Gilbert Strang, a renowned mathematician and professor at MIT. The lectures are clear and concise, and the course covers a wide range of topics in linear algebra. The course is also available on YouTube that inlcudes an interview with Prof. Strang.</p>"},{"location":"pages/online_resources/#3blue1brown","title":"3blue1brown","text":"<p>3blue1brown is a YouTube channel that provides visual explanations of various mathematical concepts, including linear algebra. The videos are engaging and can help you understand the material better. The channel is run by Grant Sanderson, who has a knack for making complex topics accessible and enjoyable. The videos are well-produced and often include animations that illustrate the concepts being discussed. The Essence of linear algebra is definitely worth watching,   but it does not cover all the material in the course.</p>"},{"location":"pages/online_resources/#sheldnon-axler-linear-algebra-done-right","title":"Sheldnon Axler: Linear Algebra Done Right","text":"<p>In terms of books, I would recommend \"Linear Algebra Done Right\" by Sheldon Axler. This book takes a different approach to linear algebra, focusing on the concepts and ideas rather than the computational aspects. It is well-written and provides a clear understanding of the subject. The book is available for free online here.</p>"},{"location":"pages/online_resources/#khan-academy","title":"Khan Academy","text":"<p>Khan Acedemy has a comprehensive linear algebra course that covers the basics and more advanced topics. The course includes video lectures, practice exercises, and quizzes to help you reinforce your understanding of the material. The platform is user-friendly and allows you to learn at your own pace. There is also a YouTube playlist that covers the same material.</p>"}]}